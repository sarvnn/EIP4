{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CODE9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarvnn/EIP4/blob/master/CODE9-ACC9956.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "outputId": "7424a15c-f5f7-4dc7-bdb7-e71b9754ba76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "outputId": "5c59eeba-5eeb-4572-f6ad-10d8daa475d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "d802e691-68ad-4354-cdc8-9e0c7397523e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[9])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f697a012710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOR0lEQVR4nO3df4wc9XnH8c/jy/kHBgcbO9fDMQFi\naGVF7UGvJgk0JaJBxKpiHCSKJVKHoh5pcASRqUKhUkjTRE5VQCRKrRyxGyelIKRAsSIrwXHTGhJw\nfEY2/tVgQm3Z17MPcCMbSuw7++kfN9AL7Hz3vDu7s3fP+yWtdneenZ3HK39uZve7O19zdwGY+CaV\n3QCA5iDsQBCEHQiCsANBEHYgiHc1c2OTbYpP1fRmbhII5dd6XSf8uFWq1RV2M7tG0gOS2iR9291X\nph4/VdN1mV1VzyYBJGz2jbm1mg/jzaxN0jclfVzSAklLzWxBrc8HoLHqec++UNKL7v6Su5+Q9Iik\nxcW0BaBo9YR9rqQDo+4fzJb9BjPrMbM+M+sb0vE6NgegHg3/NN7de92929272zWl0ZsDkKOesPdL\nmjfq/nuzZQBaUD1h3yLpIjO7wMwmS7pB0rpi2gJQtJqH3tx92MyWS/qRRobe1rj7rsI6A1CousbZ\n3X29pPUF9QKggfi6LBAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrAD\nQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBNHXKZmC8OOenM5P1SebJ+ssf/lWR7RSCPTsQ\nBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4O0J6YXV3sr7lvAeS9Q89dWuyfqG2nXZPjVZX2M1sn6Rj\nkk5KGnb39CsIoDRF7Nk/6u6vFPA8ABqI9+xAEPWG3SU9aWZbzayn0gPMrMfM+sysb0jH69wcgFrV\nexh/hbv3m9l7JG0ws/90902jH+DuvZJ6JWmGzUr/egBAw9S1Z3f3/ux6UNLjkhYW0RSA4tUcdjOb\nbmZnvXlb0tWSdhbVGIBi1XMY3yHpcTN783n+xd1/WEhXQAFeWJV/oLnl6vuT6x47lX7HOeM/ptXU\nU5lqDru7vyTp9wrsBUADMfQGBEHYgSAIOxAEYQeCIOxAEPzEFRPWlZfsya2dNWlyct3P7r8mWZ/9\nrWdq6qlM7NmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2Se4Nxanzycye8V/JevH/7QtWR8eOHTa\nPRVl8LMfTta/1pH/M9Z/Pvq+5Lr/89fnJeuT9Gqy3orYswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxA\nEIyzT3A3rvxBsn7TjAPJ+h///l8m61N/UN44+7Jb1yfrXVOm5Nb+4stLkuvOemr8/V69GvbsQBCE\nHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wT3MCJs5P1U9qfrA9PsyLbOS2n/uiSZH3xmd9I1oc8f1rl\n4anl/bvKUnXPbmZrzGzQzHaOWjbLzDaY2d7semZj2wRQr7Ecxn9H0tunx7hT0kZ3v0jSxuw+gBZW\nNezuvknSkbctXixpbXZ7raRrC+4LQMFqfc/e4e4D2e1DkjryHmhmPZJ6JGmqzqhxcwDqVfen8e7u\nkjxR73X3bnfvblf+DxMANFatYT9sZp2SlF0PFtcSgEaoNezrJC3Lbi+T9EQx7QBolKrv2c3sYUlX\nSpptZgclfVHSSkmPmtnNkvZLur6RTSJt79cvy609fk56LHrVry5O1s9+tj9ZH05W09rOfney/sod\nryfr574r/bbw8/+df175jtVbk+vmvi8dx6qG3d2X5pSuKrgXAA3E12WBIAg7EARhB4Ig7EAQhB0I\ngp+4jgNtvz0/Wf/en6zKrf2vDyXXfezuq5P1aQd+nqzXY+8/XpCs77z0wWT9x2+clX7+Pzh+2j1N\nZOzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtlbgF/elazfsDo97XL3lJO5td/54W3JdS/+18aN\no0vSvr/7UG6t7yP3VVk7/d/zC9/+82R9rn5W5fljYc8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ew\nzl4Aa5+crA8s707W++5In+653dqS9SHP/5v9ya7nkuuu+1r+OLgkzf/S9mR90m+9J1n/xKJnc2tt\nSk+b3PWz9Dj6eSsZRz8d7NmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAhzb97ktDNsll9mE2/y18Of\ny58aWJI23/lAXc8/qcrf5O8enZtbu3HGgbq2fdeh/OmgJelj796VrH902mu5tc3H25PrfuXC9O/8\n8U6bfaOO+pGKX2Coumc3szVmNmhmO0ctu8fM+s1sW3ZZVGTDAIo3lsP470i6psLy+929K7usL7Yt\nAEWrGnZ33yTpSBN6AdBA9XxAt9zMns8O82fmPcjMesysz8z6hsTcW0BZag37Kknvl9QlaUDSvXkP\ndPded+929+52TalxcwDqVVPY3f2wu59091OSHpS0sNi2ABStprCbWeeou0sk7cx7LIDWUHWc3cwe\nlnSlpNmSDkv6Yna/S5JL2ifpFncfqLax8TzO/vJn8n/3/fTfpMfRq82RvntoerJ+9x23JOtTXz2R\nW5vz1X3Jdf/p/CeT9WqqfQfglE7l1k5W+b+36dfp+dcfuO6T6W1v35OsT0SpcfaqJ69w96UVFq+u\nuysATcXXZYEgCDsQBGEHgiDsQBCEHQiCU0mP0YI/yx/GWfd6R3Ldr/ZWGtD4f533pk+JfIY2J+sp\nr6743WT989/4w2T9/nOfqnnb1bRZ+lTSf7XjumT93O27i2xnwmPPDgRB2IEgCDsQBGEHgiDsQBCE\nHQiCsANBMM4+Rlt/tCC3duSR2cl1O39R3tTCb3RMTdY/N+ffqjxD+nTPH/zb5cn67O2vV3n+fPNe\n7E/WT9b8zDGxZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnH6PzvpQ/Vl72eG/bnDm5tYPXDSfX\nnd+enqXnoWOdyfrsbz2TrNej7Nd1omHPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM4+AexdMT+3\ntueqryfXfeZ4+vfqj34ifV556ZdV6mgVVffsZjbPzH5iZrvNbJeZ3ZYtn2VmG8xsb3Y9s/HtAqjV\nWA7jhyWtcPcFkj4o6VYzWyDpTkkb3f0iSRuz+wBaVNWwu/uAuz+X3T4maY+kuZIWS1qbPWytpGsb\n1SSA+p3We3YzO1/SJZI2S+pw94GsdEhSxQnPzKxHUo8kTdUZtfYJoE5j/jTezM6U9H1Jt7v70dE1\nd3dJXmk9d+919253725X+kcXABpnTGE3s3aNBP0hd38sW3zYzDqzeqekwca0CKAIVQ/jzcwkrZa0\nx93vG1VaJ2mZpJXZ9RMN6RBqW3Bxsv7lJY/k1k56xQOut9y07jPJ+vwXnk3WMX6M5T375ZI+JWmH\nmW3Llt2lkZA/amY3S9ov6frGtAigCFXD7u5PS7Kc8lXFtgOgUfi6LBAEYQeCIOxAEIQdCIKwA0Hw\nE9dx4PrH/j1ZX3Jm/veZLn32puS6829nHD0K9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7OPA\nV564LllfemP+6aKnrZ9RdDsYp9izA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ5lXOK16kGTbLLzNO\nSAs0ymbfqKN+pOLZoNmzA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQVcNuZvPM7CdmttvMdpnZbdny\ne8ys38y2ZZdFjW8XQK3GcvKKYUkr3P05MztL0lYz25DV7nf3f2hcewCKMpb52QckDWS3j5nZHklz\nG90YgGKd1nt2Mztf0iWSNmeLlpvZ82a2xsxm5qzTY2Z9ZtY3pON1NQugdmMOu5mdKen7km5396OS\nVkl6v6Qujez57620nrv3unu3u3e3a0oBLQOoxZjCbmbtGgn6Q+7+mCS5+2F3P+nupyQ9KGlh49oE\nUK+xfBpvklZL2uPu941a3jnqYUsk7Sy+PQBFGcun8ZdL+pSkHWa2LVt2l6SlZtYlySXtk3RLQzoE\nUIixfBr/tKRKv49dX3w7ABqFb9ABQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIO\nBEHYgSAIOxAEYQeCaOqUzWb2sqT9oxbNlvRK0xo4Pa3aW6v2JdFbrYrs7X3uPqdSoalhf8fGzfrc\nvbu0BhJatbdW7Uuit1o1qzcO44EgCDsQRNlh7y15+ymt2lur9iXRW62a0lup79kBNE/Ze3YATULY\ngSBKCbuZXWNmvzCzF83szjJ6yGNm+8xsRzYNdV/Jvawxs0Ez2zlq2Swz22Bme7PrinPsldRbS0zj\nnZhmvNTXruzpz5v+nt3M2iS9IOljkg5K2iJpqbvvbmojOcxsn6Rudy/9Cxhm9hFJr0n6rrt/IFv2\n95KOuPvK7A/lTHf/Qov0do+k18qexjubrahz9DTjkq6V9GmV+Nol+rpeTXjdytizL5T0oru/5O4n\nJD0iaXEJfbQ8d98k6cjbFi+WtDa7vVYj/1maLqe3luDuA+7+XHb7mKQ3pxkv9bVL9NUUZYR9rqQD\no+4fVGvN9+6SnjSzrWbWU3YzFXS4+0B2+5CkjjKbqaDqNN7N9LZpxlvmtatl+vN68QHdO13h7pdK\n+rikW7PD1ZbkI+/BWmnsdEzTeDdLhWnG31Lma1fr9Of1KiPs/ZLmjbr/3mxZS3D3/ux6UNLjar2p\nqA+/OYNudj1Ycj9vaaVpvCtNM64WeO3KnP68jLBvkXSRmV1gZpMl3SBpXQl9vIOZTc8+OJGZTZd0\ntVpvKup1kpZlt5dJeqLEXn5Dq0zjnTfNuEp+7Uqf/tzdm36RtEgjn8j/UtLdZfSQ09eFkrZnl11l\n9ybpYY0c1g1p5LONmyWdI2mjpL2SfixpVgv19j1JOyQ9r5FgdZbU2xUaOUR/XtK27LKo7Ncu0VdT\nXje+LgsEwQd0QBCEHQiCsANBEHYgCMIOBEHYgSAIOxDE/wElMTCIuxoFJAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train[:10]\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFR0F9j0xVp2",
        "colab_type": "code",
        "outputId": "4f54f336-2140-4727-ec39-23dd4ac27442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "044ef957-bc5e-49c8-aa7c-b139ffd2fd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu')) #22\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 4, 4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 24, 24, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 24, 24, 10)        170       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,060\n",
            "Trainable params: 13,848\n",
            "Non-trainable params: 212\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "bd0400ef-9153-4474-ea5c-df91f98ba961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=40, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.1157 - acc: 0.9511 - val_loss: 0.0398 - val_acc: 0.9886\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.1057 - acc: 0.9535 - val_loss: 0.0282 - val_acc: 0.9920\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.1002 - acc: 0.9538 - val_loss: 0.0304 - val_acc: 0.9912\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0959 - acc: 0.9559 - val_loss: 0.0235 - val_acc: 0.9937\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0935 - acc: 0.9556 - val_loss: 0.0228 - val_acc: 0.9929\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0930 - acc: 0.9571 - val_loss: 0.0222 - val_acc: 0.9932\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0901 - acc: 0.9566 - val_loss: 0.0215 - val_acc: 0.9940\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0881 - acc: 0.9574 - val_loss: 0.0211 - val_acc: 0.9937\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0849 - acc: 0.9592 - val_loss: 0.0181 - val_acc: 0.9948\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0871 - acc: 0.9575 - val_loss: 0.0232 - val_acc: 0.9932\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0836 - acc: 0.9592 - val_loss: 0.0204 - val_acc: 0.9940\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0862 - acc: 0.9577 - val_loss: 0.0194 - val_acc: 0.9953\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0852 - acc: 0.9581 - val_loss: 0.0174 - val_acc: 0.9949\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0848 - acc: 0.9586 - val_loss: 0.0190 - val_acc: 0.9946\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0832 - acc: 0.9586 - val_loss: 0.0198 - val_acc: 0.9945\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0820 - acc: 0.9578 - val_loss: 0.0184 - val_acc: 0.9949\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0817 - acc: 0.9588 - val_loss: 0.0180 - val_acc: 0.9944\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0795 - acc: 0.9601 - val_loss: 0.0177 - val_acc: 0.9947\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0807 - acc: 0.9597 - val_loss: 0.0189 - val_acc: 0.9945\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0815 - acc: 0.9600 - val_loss: 0.0204 - val_acc: 0.9948\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0779 - acc: 0.9601 - val_loss: 0.0176 - val_acc: 0.9951\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0814 - acc: 0.9587 - val_loss: 0.0197 - val_acc: 0.9941\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0785 - acc: 0.9604 - val_loss: 0.0189 - val_acc: 0.9949\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0792 - acc: 0.9596 - val_loss: 0.0190 - val_acc: 0.9949\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0789 - acc: 0.9594 - val_loss: 0.0182 - val_acc: 0.9951\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0764 - acc: 0.9602 - val_loss: 0.0191 - val_acc: 0.9945\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0782 - acc: 0.9609 - val_loss: 0.0199 - val_acc: 0.9947\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0769 - acc: 0.9601 - val_loss: 0.0207 - val_acc: 0.9942\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0776 - acc: 0.9604 - val_loss: 0.0177 - val_acc: 0.9950\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0787 - acc: 0.9585 - val_loss: 0.0179 - val_acc: 0.9953\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0767 - acc: 0.9608 - val_loss: 0.0190 - val_acc: 0.9947\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0753 - acc: 0.9604 - val_loss: 0.0202 - val_acc: 0.9941\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0766 - acc: 0.9599 - val_loss: 0.0188 - val_acc: 0.9952\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0760 - acc: 0.9608 - val_loss: 0.0175 - val_acc: 0.9952\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0758 - acc: 0.9617 - val_loss: 0.0171 - val_acc: 0.9959\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0783 - acc: 0.9595 - val_loss: 0.0187 - val_acc: 0.9951\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0763 - acc: 0.9605 - val_loss: 0.0178 - val_acc: 0.9959\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0745 - acc: 0.9604 - val_loss: 0.0191 - val_acc: 0.9948\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0762 - acc: 0.9597 - val_loss: 0.0185 - val_acc: 0.9955\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0738 - acc: 0.9613 - val_loss: 0.0173 - val_acc: 0.9956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f68d5100588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "outputId": "1aab4b98-fc22-47a1-b2d1-6a890c07cd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.017316753494666045, 0.9956]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2qDl21ozBnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}